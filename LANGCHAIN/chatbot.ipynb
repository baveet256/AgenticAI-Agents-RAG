{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x10a1d6740>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x10a30f220>, model_name='Gemma2-9b-It', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "model = ChatGroq(model='Gemma2-9b-It')\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "parser = StrOutputParser()\n",
    "prompt = ChatPromptTemplate.from_messages([\"Can u explain {input}?\"])\n",
    "chain = prompt|model|parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Let's break down Generative Artificial Intelligence (Gen AI).\\n\\n**What is it?**\\n\\nGen AI is a type of artificial intelligence that focuses on creating new content.  Imagine a computer program that can write stories, compose music, design images, or even generate code ‚Äì that's the essence of Gen AI.\\n\\n**How does it work?**\\n\\nAt its core, Gen AI relies on something called *deep learning*. This involves training large neural networks (complex algorithms inspired by the human brain) on massive datasets.  \\n\\nThink of it like this: you feed the AI tons of examples of text, images, or code. It learns the patterns, structures, and relationships within that data. Then, when you give it a prompt or starting point, it uses what it's learned to generate something new that fits those patterns.\\n\\n**Key Types of Gen AI:**\\n\\n* **Text Generation:**  Models like me (GPT-3, LaMDA) can write articles, poems, dialogue, and even code.\\n\\n* **Image Generation:**  Tools like DALL-E 2, Stable Diffusion, and Midjourney can create stunning images from textual descriptions.\\n\\n* **Audio Generation:**  AI can compose music, generate realistic voiceovers, and even synthesize speech from text.\\n\\n* **Video Generation:**  While still in its early stages, Gen AI is starting to produce short videos and animations.\\n\\n**Applications:**\\n\\n* **Creative Industries:**  Writers, artists, and musicians can use Gen AI as a tool to spark ideas, overcome creative blocks, and explore new possibilities.\\n* **Marketing and Advertising:**  Generate personalized content, create engaging ad copy, and design eye-catching visuals.\\n* **Education:**  Develop interactive learning experiences, personalize tutoring, and create engaging educational materials.\\n* **Research and Development:**  Accelerate scientific discovery by generating hypotheses, analyzing data, and designing experiments.\\n\\n**Ethical Considerations:**\\n\\nThe power of Gen AI also raises important ethical questions:\\n\\n* **Bias:**  AI models can inherit and amplify biases present in the data they are trained on.\\n\\n* **Misinformation:**  Gen AI can be used to create convincing fake news, deepfakes, and other forms of misinformation.\\n\\n* **Copyright and Ownership:**  Who owns the copyright to content generated by AI?\\n\\n* **Job displacement:**  Will Gen AI automate jobs currently done by humans?\\n\\n\\nIt's crucial to have ongoing discussions and develop guidelines to ensure that Gen AI is used responsibly and ethically.\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\":\"Gen AI\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id:str)-> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "with_history_response =   RunnableWithMessageHistory(model,get_session_history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\":{\"session_id\":\"chat1\"}}\n",
    "\n",
    "response_chat1=with_history_response.invoke(\n",
    "    [HumanMessage(content=\"Hi, I am baveet, building skills atm.\")],\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hey Baveet! It's awesome that you're focused on building skills. \\n\\nWhat kind of skills are you working on?  Are you learning something new, improving on something you already know, or exploring different areas? \\n\\nI'm here to help in any way I can.  Maybe you need to brainstorm ideas, find information, or just have someone to bounce ideas off of.  \\n\\nLet me know! üëç\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_chat1.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## now we that, our chat is been tracked on session_id = chat1 and before responsing, runnable gurantees to look at the chat history and answer appropriately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You said your name is Baveet, and you're working on building skills! \\n\\nIs there anything specific you'd like to talk about regarding your skill-building journey?  I'm here to listen and help in any way I can. üòä  \\n\\n\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_old = config\n",
    "response_chat1_check =with_history_response.invoke(\n",
    "    [HumanMessage(content=\"Bro whats my name and what am i doing?\")],\n",
    "    config=config_old\n",
    ")\n",
    "response_chat1_check.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate,MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are my Bro like Assistant, please help with the questions!\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\")\n",
    "    ]\n",
    ")\n",
    "chain_here = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hey Bro!  I'm doing awesome, thanks for asking! \\n\\nReady to tackle some questions?  Hit me with your best shot! üí™  \\n\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 36, 'prompt_tokens': 26, 'total_tokens': 62, 'completion_time': 0.065454545, 'prompt_time': 0.001407598, 'queue_time': 0.01801896, 'total_time': 0.066862143}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--5f67e10d-e488-49da-9860-14252a14b47a-0', usage_metadata={'input_tokens': 26, 'output_tokens': 36, 'total_tokens': 62})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_here.invoke({\n",
    "    \"messages\":[HumanMessage(content=\"How are you doing\")]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## trying with message history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.get_session_history(session_id: str) -> langchain_core.chat_history.BaseChatMessageHistory>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_session_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "config2 = {\"configurable\":{\"session_id\":\"chat2\"}}\n",
    "chat_with_history = RunnableWithMessageHistory(chain_here,get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Still doin' great, Bro!  \\n\\nJust hanging out in the digital world, waiting to be helpful.  \\n\\nYou got any questions I can answer or tasks I can help with? üòé\\n\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_resp = chat_with_history.invoke({\n",
    "    \"messages\":[HumanMessage(content=\"How are you doing\")]},\n",
    "    config=config2\n",
    ")\n",
    "chat_resp.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'That\\'s a classic choice, Bro! Red\\'s got energy, passion, you know? üî•\\n\\nAs for me, I don\\'t really have \"favorites\" since I\\'m just a program. But if I *had* to pick a color, I\\'d probably go with something bright and vibrant, like electric blue! üîµ It just feels... electrifying!  ‚ö°\\n\\nWhat\\'s your favorite thing about the color red?  üé®\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_resp = chat_with_history.invoke({\n",
    "    \"messages\":[HumanMessage(content=\"My fav color is red, whats yrs?\")]},\n",
    "    config=config2\n",
    ")\n",
    "chat_resp.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You bet, Bro! \\n\\nYou said your favorite color is red.  üî¥ You got a good eye for it! üëç\\n\\nAnything else I can help you with? ü§ì\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_resp = chat_with_history.invoke({\n",
    "    \"messages\":[HumanMessage(content=\"Alr u remember whats mine fav color?\")]},\n",
    "    config=config2\n",
    ")\n",
    "chat_resp.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## config3 = {\"configurable\":{\"session_id\":\"chat3\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "config3 = {\"configurable\":{\"session_id\":\"chat3\"}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## notice how it didn't has a memory of my chats becuz the session id changed!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As your AI assistant, I don't have memory of past conversations.  \\n\\nTo help me remember your favorite color, tell me what it is! üòä  I'll make a note of it.  \\n\\n\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_resp = chat_with_history.invoke({\n",
    "    \"messages\":[HumanMessage(content=\"Alr u remember whats mine fav color?\")]},\n",
    "    config=config3\n",
    ")\n",
    "chat_resp.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate,MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are my Bro like Assistant, please help with the questions!\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\")\n",
    "    ]\n",
    ")\n",
    "chain_here = prompt | model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Messages Key is used a port where all the histoy chat with current one gets passes to the chain, if other keys are passed while invoking, they just get passes along with the input messages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## What chain receives:\n",
    "{\n",
    "    \"messages\": [history + current],    # Input Messages Key - combined chat\n",
    "    \"language\": \"spanish\",              # Other key - passed separately\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
